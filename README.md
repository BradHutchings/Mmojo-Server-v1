## Mmojo Server
Based on [llama.cpp](https://github.com/ggml-org/llama.cpp).

Brad Hutchings<br/>
brad@bradhutchings.com

---
### Project Goals

The main project goals are:

1. Create `mmojo-server-one` executables that run anwhere:
   - x86_64 Windows
   - x86_64 Linux
   - ARM Windows
   - ARM Linux
   - ARM MacOS

   This was the main goal of the [llamafile project](https://github.com/Mozilla-Ocho/llamafile) project, which inspires me.

   I also create `mmojo-server` executables targeted at specific platforms and operating systems, and supporting GPU and NPU computation as llama.cpp does with the same source code, similar packaging, and deployment conventions.

   **Easy button:** Start with `mmojo-server-one`. When you get your installation working, build and install `mmojo-server` optimized for your device.

3. Keep the underlying [llama.cpp](https://github.com/ggml-org/llama.cpp) base up-to-date. This was the (and my) main complaint about llamafile when that project was being updated regularly. Without regular (read "at least weekly") updates to the llama.cpp base, new models and modes of use were not supported.

4. Present the **Mmojo Completion** user interface. You shouldn't have to pretend to chat to get knowledge from an LLM.

In addition, these general goals inform development:

* I want to use the MIT license as used by llama.cpp.

* GPU support is important to me, and can be handled by platform specific builds of llama.cpp. It's complicated to make work with a cross-platform and cross-architecture build. CPU inference is quite adequate for many private end-user applications. Base-level ARM and x86 tuned CPU inference is implemented.

* The ability to package support files, such as a custom web UI into the executable file is important to me. This is implemented for `mmojo-server-one` builds.

* The ability to package default arguments, in an "args" file, into the executable file is important to me. This is implemented for `mmojo-server-one` builds.

* The ability to read arguments from a file adjacent to the executable file is important to me. This is implemented for `mmojo-server-one` and `mojo-server` builds.

* The ability to package a gguf model into the executable file is important to me. This is implemented for `mmojo-server-one` and `mojo-server` builds.

* Industry standard OpenAI API.

I welcome any of my changes being implemented in the official llama.cpp.

---
### Documentation
Follow these guides in order to build, package, and deploy Mmojo Server:
- My start-to-finish guide for building Mmojo Server with Cosmo [starts here](docs/0-Prepare-Build-Environment.md).

<!--
I am in the process of rewriting the documentation to use a multi-machine build system, shared builds and products on an SMB share, and get more optimal performance out of targeted builds. That journey [starts here](docs/NEW-10-Prepare-Build-Environment.md) and currently leads into the current documentation after the buid environment is prepared.
-->

---
### Modifications to llama.cpp

To get this from the llama.cpp source base, there are few files that need to be modified:

1. [common/arg-mmojo.cpp](common/arg-mmojo.cpp)
   - Added a parameter for sleep after each batch.
   - Original file, [common/arg.cpp](common/arg.cpp), is preserved.

2. [common/common-mmojo.cpp](common/common-mmojo.cpp)
   - Location of cache directory for COSMOCC builds.
   - Original file, [common/common.cpp](common/common.cpp), is preserved.

4. [common/common.h](common/common.h)
   - Added parameters for sleep after each batch and default UI endpoint to `common_params`.
   - This original file is modified in this repo, as shown in the snippet at [common/common-mmojo.h](common/common-mmojo.h). This adjustment should be moved to the `sed` commands in the build instructions.

6. [tools/server/server-mmojo.cpp](tools/server/server-mmojo.cpp)
   - Support embedded or adjacent "args" file, fix Cosmo name conflict with "defer" task member, add additional meta data to `model_meta`, stream reporting of evaluating progress, and more.
   - Original file, [tools/server/server.cpp](tools/server/server.cpp), is preserved.

8. [completion-ui](completion-ui) -- Default UI is Mmojo Completion.

9. [tools/server/public/loading-mmojo.html](tools/server/public/loading-mmojo.html)
   - Loading page matches Mmojo Completion theme.

---
### Reference

Here are some projects and pages you should be familiar with if you want to get the most out of `mmojo-server`:
- [llama.cpp](https://github.com/ggml-org/llama.cpp) - Georgi Gerganov and his team are the rock stars who are making the plumbing so LLMs can be available for developers of all kinds. The `llama.cpp` project is the industry standard for inference. I only fork it here because I want to make it a little better for my applications while preserving all its goodness.
- [llamafile](https://github.com/Mozilla-Ocho/llamafile) - `Llamafile` lets you distribute and run LLMs with a single file. It is a Mozilla Foundation project that brough the Cosmopolitan C Library and llama.cpp together. It has some popular GPU support. It is based on an older version of llama.cpp and does not support all of the latest models supported by llama.cpp. Llamafile is an inspiration for this project.
- [Cosmopolitan Libc](https://github.com/jart/cosmopolitan) - `Cosmopolitan` is a project for building cross-platform binaries that run on x86_64 and ARM architectures, supporting Linux, Windows, macOS, and other operating systems. Like `llamafile`, I use Cosmo compile cross-platform executables of `llama.cpp` targets, including `llama-server`.
- [Actually Portable Executable (APE) Specification](https://github.com/jart/cosmopolitan/blob/master/ape/specification.md) - Within the Cosmopolitan Libc repo is documentation about how the cross CPU, cross platform executable works.
- [Brad's LLMs](https://huggingface.co/bradhutchings/Brads-LLMs) - I share private local LLMs built with `llamafile` in a Hugging Face repo.

---
### To Do List

In no particular order of importance, these are the things that bother me:
- GPU support without a complicated kludge, and that can support all supported platform / CPU / GPU triads. Perhaps a plugin system with shared library dispatch? Invoking dev tools on Apple Metal like llamafile does is "complicated".
- Code signing instructions. Might have to sign executables within the zip package, plus the package itself.
- The `mmojo-args` (from `cosmo-args`) thing is cute, but it might be easier for users as a yaml file. Key value pairs. Flags can be keys with null values.
- ~~Copy the `cosmo_args` function into `server.cpp` so it could potentially be incorporated upstream in non-Cosmo builds. `common/arg2.cpp` might be a good landing spot. License in [Cosmo source code](https://github.com/jart/cosmopolitan/blob/master/tool/args/args2.c) appears to be MIT compatible with attribution.~~ Implemented.
- ~~Clean up remaining build warnings, either by fixing source (i.e. Cosmo) or finding the magical compiler flags.~~ Implemented.
- ~~Write docs for a Deploying step. It should address the args file, removing the extra executable depending on platform, models, host, port. context size.~~ Mostly implemented.
- ~~Package gguf file into executable file. The zip item needs to be aligned for mmap. There is a zipalign.c tool source in llamafile that seems loosely inspired by the Android zipalign too. I feel like there should be a more generic solution for this problem.~~ Implemented.
- ~~Make a `.gitattributes` file so we can set the default file to be displayed and keep the README.md from llama.cpp. This will help in syncing changes continually from upstream. Reference: https://git-scm.com/docs/gitattributes~~ -- This doesn't actually work.
- ~~Cosmo needs libssl and libcrypto. Building these from scratch gets an error about Cosco not liking assembly files. Sort this out.~~ Implemented.
- ~~The `--ctx-size` parameter doesn't seem quite right given that new models have the training (or max) context size in their metadata. That size should be used subject to a maximum in a passed parameter. E.g. So a 128K model can run comfortably on a smaller device.~~ `--ctx-size 0` uses the training size.



















